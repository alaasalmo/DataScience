{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop commands"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1-Go to CMD\n",
    "2->Start-all\n",
    "3- Get the files from data folder\n",
    "  >hadoop fs -mkdir /finance/\n",
    "  >hadoop fs -mkdir \"/finance/stock=AAPL\"\n",
    "  >hadoop fs -mkdir \"/finance/stock=AIG\"\n",
    "  >hadoop fs -mkdir \"/finance/stock=AMZN\"\n",
    "  >hadoop fs -mkdir \"/finance/stock=BA\"\n",
    "  >hadoop fs -mkdir \"/finance/stock=AXP\"\n",
    "\n",
    "4-Load the file from GitHub to Hadoop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import pyspark\n",
    "import random\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "spark = SparkSession.builder.appName('stocks').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|stock|\n",
      "+-----+\n",
      "|  AXP|\n",
      "| AAPL|\n",
      "|  AIG|\n",
      "|   BA|\n",
      "| AMZN|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('hdfs://localhost:9000/finance',inferSchema=True,header=True)\n",
    "df.select('stock').distinct().show()\n",
    "#groupBy=df.select('country','city').groupBy('country').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- open: double (nullable = true)\n",
      " |-- close: double (nullable = true)\n",
      " |-- stock: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|stock|count|\n",
      "+-----+-----+\n",
      "|  AXP| 1258|\n",
      "| AAPL| 1258|\n",
      "|  AIG| 1258|\n",
      "|   BA| 1258|\n",
      "| AMZN| 1258|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"date\",\"stock\").groupBy(\"stock\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-----+-----+\n",
      "|              dateR| open|close|stock|\n",
      "+-------------------+-----+-----+-----+\n",
      "|2003-01-02 00:00:00|14.36| 14.8| AAPL|\n",
      "|2003-01-03 00:00:00| 14.8| 14.9| AAPL|\n",
      "|2003-01-06 00:00:00|15.03| 14.9| AAPL|\n",
      "|2003-01-07 00:00:00|14.79|14.85| AAPL|\n",
      "|2003-01-08 00:00:00|14.58|14.55| AAPL|\n",
      "|2003-01-09 00:00:00|14.62|14.68| AAPL|\n",
      "|2003-01-10 00:00:00|14.58|14.72| AAPL|\n",
      "|2003-01-13 00:00:00| 14.9|14.63| AAPL|\n",
      "|2003-01-14 00:00:00|14.69|14.61| AAPL|\n",
      "|2003-01-15 00:00:00|14.59|14.43| AAPL|\n",
      "|2003-01-16 00:00:00|14.21|14.62| AAPL|\n",
      "|2003-01-17 00:00:00|14.56| 14.1| AAPL|\n",
      "|2003-01-21 00:00:00|14.21|14.02| AAPL|\n",
      "|2003-01-22 00:00:00|13.98|13.88| AAPL|\n",
      "|2003-01-23 00:00:00|14.05|14.17| AAPL|\n",
      "|2003-01-24 00:00:00|14.24| 13.8| AAPL|\n",
      "|2003-01-27 00:00:00|13.68|14.13| AAPL|\n",
      "|2003-01-28 00:00:00|14.24|14.58| AAPL|\n",
      "|2003-01-29 00:00:00|14.24|14.58| AAPL|\n",
      "|2003-01-30 00:00:00|14.98|14.32| AAPL|\n",
      "+-------------------+-----+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df.withColumn(\"datetime\", col(\"datetime\").cast(\"timestamp\")\n",
    "#df.withColumn(\"date\", toTimeStamp(df(\"date\"))).groupBy(\"stock\").max(\"date\").show()\n",
    "#df.select(\"date\").show()\n",
    "df2=df.withColumnRenamed('date',('dateR'))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Casting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-----+-----+-------------------+\n",
      "|               date| open|close|stock|              Ndate|\n",
      "+-------------------+-----+-----+-----+-------------------+\n",
      "|2003-01-02 00:00:00|14.36| 14.8| AAPL|2003-01-02 00:00:00|\n",
      "|2003-01-03 00:00:00| 14.8| 14.9| AAPL|2003-01-03 00:00:00|\n",
      "|2003-01-06 00:00:00|15.03| 14.9| AAPL|2003-01-06 00:00:00|\n",
      "|2003-01-07 00:00:00|14.79|14.85| AAPL|2003-01-07 00:00:00|\n",
      "|2003-01-08 00:00:00|14.58|14.55| AAPL|2003-01-08 00:00:00|\n",
      "|2003-01-09 00:00:00|14.62|14.68| AAPL|2003-01-09 00:00:00|\n",
      "|2003-01-10 00:00:00|14.58|14.72| AAPL|2003-01-10 00:00:00|\n",
      "|2003-01-13 00:00:00| 14.9|14.63| AAPL|2003-01-13 00:00:00|\n",
      "|2003-01-14 00:00:00|14.69|14.61| AAPL|2003-01-14 00:00:00|\n",
      "|2003-01-15 00:00:00|14.59|14.43| AAPL|2003-01-15 00:00:00|\n",
      "|2003-01-16 00:00:00|14.21|14.62| AAPL|2003-01-16 00:00:00|\n",
      "|2003-01-17 00:00:00|14.56| 14.1| AAPL|2003-01-17 00:00:00|\n",
      "|2003-01-21 00:00:00|14.21|14.02| AAPL|2003-01-21 00:00:00|\n",
      "|2003-01-22 00:00:00|13.98|13.88| AAPL|2003-01-22 00:00:00|\n",
      "|2003-01-23 00:00:00|14.05|14.17| AAPL|2003-01-23 00:00:00|\n",
      "|2003-01-24 00:00:00|14.24| 13.8| AAPL|2003-01-24 00:00:00|\n",
      "|2003-01-27 00:00:00|13.68|14.13| AAPL|2003-01-27 00:00:00|\n",
      "|2003-01-28 00:00:00|14.24|14.58| AAPL|2003-01-28 00:00:00|\n",
      "|2003-01-29 00:00:00|14.24|14.58| AAPL|2003-01-29 00:00:00|\n",
      "|2003-01-30 00:00:00|14.98|14.32| AAPL|2003-01-30 00:00:00|\n",
      "+-------------------+-----+-----+-----+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "dfCast=df.withColumn(\"Ndate\", df[\"date\"].cast(StringType()))\n",
    "dfCast.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+----+-----+---+\n",
      "|               date|stock|year|month|day|\n",
      "+-------------------+-----+----+-----+---+\n",
      "|2003-01-02 00:00:00| AAPL|2003|    1|  2|\n",
      "|2003-01-03 00:00:00| AAPL|2003|    1|  3|\n",
      "|2003-01-06 00:00:00| AAPL|2003|    1|  6|\n",
      "|2003-01-07 00:00:00| AAPL|2003|    1|  7|\n",
      "|2003-01-08 00:00:00| AAPL|2003|    1|  8|\n",
      "|2003-01-09 00:00:00| AAPL|2003|    1|  9|\n",
      "|2003-01-10 00:00:00| AAPL|2003|    1| 10|\n",
      "|2003-01-13 00:00:00| AAPL|2003|    1| 13|\n",
      "|2003-01-14 00:00:00| AAPL|2003|    1| 14|\n",
      "|2003-01-15 00:00:00| AAPL|2003|    1| 15|\n",
      "|2003-01-16 00:00:00| AAPL|2003|    1| 16|\n",
      "|2003-01-17 00:00:00| AAPL|2003|    1| 17|\n",
      "|2003-01-21 00:00:00| AAPL|2003|    1| 21|\n",
      "|2003-01-22 00:00:00| AAPL|2003|    1| 22|\n",
      "|2003-01-23 00:00:00| AAPL|2003|    1| 23|\n",
      "|2003-01-24 00:00:00| AAPL|2003|    1| 24|\n",
      "|2003-01-27 00:00:00| AAPL|2003|    1| 27|\n",
      "|2003-01-28 00:00:00| AAPL|2003|    1| 28|\n",
      "|2003-01-29 00:00:00| AAPL|2003|    1| 29|\n",
      "|2003-01-30 00:00:00| AAPL|2003|    1| 30|\n",
      "+-------------------+-----+----+-----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import TimestampType\n",
    "import datetime\n",
    "newdf=df.select(\"date\",\"stock\",year(\"date\").alias('year'),month(\"date\").alias('month'),dayofmonth(\"date\").alias('day'))\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+----------+--------+\n",
      "|stock|max(year)|max(month)|max(day)|\n",
      "+-----+---------+----------+--------+\n",
      "|  AXP|     2007|        12|      31|\n",
      "| AAPL|     2007|        12|      31|\n",
      "|  AIG|     2007|        12|      31|\n",
      "|   BA|     2007|        12|      31|\n",
      "| AMZN|     2007|        12|      31|\n",
      "+-----+---------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf.groupBy('stock').max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-----+-----+--------------+\n",
      "|               date| open|close|stock|unix_timestamp|\n",
      "+-------------------+-----+-----+-----+--------------+\n",
      "|2003-01-02 00:00:00|14.36| 14.8| AAPL|    1041483600|\n",
      "|2003-01-03 00:00:00| 14.8| 14.9| AAPL|    1041570000|\n",
      "|2003-01-06 00:00:00|15.03| 14.9| AAPL|    1041829200|\n",
      "|2003-01-07 00:00:00|14.79|14.85| AAPL|    1041915600|\n",
      "|2003-01-08 00:00:00|14.58|14.55| AAPL|    1042002000|\n",
      "+-------------------+-----+-----+-----+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "df1 = df.withColumn(\"unix_timestamp\",F.unix_timestamp(df.date,'dd-MMM-yyyy HH:mm:ss.SSS z'))\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method DataFrame.printSchema of DataFrame[date: timestamp, open: double, close: double, stock: string, unix_timestamp: bigint, TimestampType: timestamp]>\n",
      "+-------------------+-----+-----+-----+--------------+-------------------+\n",
      "|date               |open |close|stock|unix_timestamp|TimestampType      |\n",
      "+-------------------+-----+-----+-----+--------------+-------------------+\n",
      "|2003-01-02 00:00:00|14.36|14.8 |AAPL |1041483600    |2003-01-02 00:00:00|\n",
      "|2003-01-03 00:00:00|14.8 |14.9 |AAPL |1041570000    |2003-01-03 00:00:00|\n",
      "+-------------------+-----+-----+-----+--------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.withColumn(\"TimestampType\",F.to_timestamp(df1[\"unix_timestamp\"]))\n",
    "print(df2.printSchema)\n",
    "df2.show(n=2,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+-------------------+\n",
      "|stock|unix_timestamp|      TimestampType|\n",
      "+-----+--------------+-------------------+\n",
      "|  AXP|    1199077200|2007-12-31 00:00:00|\n",
      "| AAPL|    1199077200|2007-12-31 00:00:00|\n",
      "|  AIG|    1199077200|2007-12-31 00:00:00|\n",
      "|   BA|    1199077200|2007-12-31 00:00:00|\n",
      "| AMZN|    1199077200|2007-12-31 00:00:00|\n",
      "+-----+--------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2=df1.groupBy(\"stock\").max(\"unix_timestamp\").withColumnRenamed('max(unix_timestamp)', 'unix_timestamp')\n",
    "#df2.show()\n",
    "\n",
    "df2.withColumn(\"TimestampType\",F.to_timestamp(df2[\"unix_timestamp\"])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "spark = SparkSession.builder.appName('abc').getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+\n",
      "|year|month|day|\n",
      "+----+-----+---+\n",
      "|1984|    1|  1|\n",
      "|1984|    1|  1|\n",
      "|1984|    1|  1|\n",
      "|1984|    1|  1|\n",
      "|1984|    1|  1|\n",
      "+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "\n",
    "elevDF = sc.parallelize([(datetime.datetime(1984, 1, 1, 0, 0), 1, 638.55),(datetime.datetime(1984, 1, 1, 0, 0), 2, 638.55),(datetime.datetime(1984, 1, 1, 0, 0), 3, 638.55),(datetime.datetime(1984, 1, 1, 0, 0), 4, 638.55),(datetime.datetime(1984, 1, 1, 0, 0), 5, 638.55)]).toDF([\"date\", \"hour\", \"value\"])\n",
    "\n",
    "elevDF.select(year(\"date\").alias('year'),month(\"date\").alias('month'),dayofmonth(\"date\").alias('day')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
