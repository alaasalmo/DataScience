{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import sparknlp\n",
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|            sentence|\n",
      "+---+--------------------+\n",
      "|  0|Hi I heard about ...|\n",
      "|  1|I wish Java could...|\n",
      "|  2|Logistic,regressi...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "\n",
    "sentenceDataFrame = spark.createDataFrame([\n",
    "    (0, \"Hi I heard about Spark hear\"),\n",
    "    (1, \"I wish Java could use case classes\"),\n",
    "    (2, \"Logistic,regression,models,are,neat\")\n",
    "], [\"id\", \"sentence\"])\n",
    "\n",
    "sentenceDataFrame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|            sentence|               words|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|Hi I heard about ...|[hi, i, heard, ab...|\n",
      "|  1|I wish Java could...|[i, wish, java, c...|\n",
      "|  2|Logistic,regressi...|[logistic, regres...|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "regexTokenized = regexTokenizer.transform(sentenceDataFrame)\n",
    "regexTokenized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------------------------------------+------+\n",
      "|sentence                           |words                                     |tokens|\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "|Hi I heard about Spark hear        |[hi, i, heard, about, spark, hear]        |6     |\n",
      "|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n",
      "|Logistic,regression,models,are,neat|[logistic, regression, models, are, neat] |5     |\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "tok=regexTokenized.select(\"sentence\", \"words\").withColumn(\"tokens\", countTokens(col(\"words\")))\n",
    "tok.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------------------------------------+------+\n",
      "|sentence                           |words                                     |tokens|\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "|Hi I heard about Spark hear        |[hi, i, heard, about, spark, hear]        |6     |\n",
      "|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n",
      "|Logistic,regression,models,are,neat|[logistic,regression,models,are,neat]     |1     |\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "tokenized = tokenizer.transform(sentenceDataFrame)\n",
    "tokenized.select(\"sentence\", \"words\")\\\n",
    "    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------------------------------------+------+------------------------------------+\n",
      "|sentence                           |words                                     |tokens|filtered                            |\n",
      "+-----------------------------------+------------------------------------------+------+------------------------------------+\n",
      "|Hi I heard about Spark hear        |[hi, i, heard, about, spark, hear]        |6     |[hi, heard, spark, hear]            |\n",
      "|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |[wish, java, use, case, classes]    |\n",
      "|Logistic,regression,models,are,neat|[logistic, regression, models, are, neat] |5     |[logistic, regression, models, neat]|\n",
      "+-----------------------------------+------------------------------------------+------+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "remover.transform(tok).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- sentence: string (nullable = true)\n",
      " |-- document: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      " |-- token: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      "\n",
      "None\n",
      "+------------------------------------------+\n",
      "|result                                    |\n",
      "+------------------------------------------+\n",
      "|[Hi, I, heard, about, Spark, hear]        |\n",
      "|[I, wish, Java, could, use, case, classes]|\n",
      "|[Logistic,regression,models,are,neat]     |\n",
      "+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df=tokenizer.transform(sentenceDataFrame)\n",
    "#df.show()\n",
    "\n",
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.embeddings import *\n",
    "\n",
    "from pyspark.sql.functions import udf, col, array\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#from sparknlp.base import LightPipeline\n",
    "\n",
    "document_assembler = DocumentAssembler().setInputCol(\"sentence\").setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer().setInputCols(\"document\").setOutputCol(\"token\")\n",
    "\n",
    "bert_pipeline = Pipeline().setStages([document_assembler,tokenizer])\n",
    "\n",
    "print(bert_pipeline.fit(sentenceDataFrame).transform(sentenceDataFrame).printSchema())\n",
    "\n",
    "df=bert_pipeline.fit(sentenceDataFrame).transform(sentenceDataFrame)\n",
    "\n",
    "df.select(\"token.result\").show(truncate=False)\n",
    "#df.withColumn('squared', col('token')).show()\n",
    "#df.withColumn(\"squared\", array(df[\"token\"])).printSchema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_base_cased download started this may take some time.\n",
      "Approximate size to download 389.2 MB\n",
      "[OK!]\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "|                text|label|            document|               token|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "|New York is the g...|    0|[[document, 0, 53...|[[token, 0, 2, Ne...|\n",
      "|The beauty of Par...|    1|[[document, 0, 26...|[[token, 0, 2, Th...|\n",
      "|The Centre Pompid...|    1|[[document, 0, 30...|[[token, 0, 2, Th...|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.embeddings import *\n",
    "\n",
    "data = [\n",
    "  (\"New York is the greatest city in the world great List \", 0),\n",
    "  (\"The beauty of Paris is vast\", 1),\n",
    "  (\"The Centre Pompidou is in Paris\", 1)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"text\",\"label\"])\n",
    "\n",
    "\n",
    "document_assembler = DocumentAssembler()\\\n",
    "  .setInputCol(\"text\")\\\n",
    "  .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer().setInputCols([\"document\"])\\\n",
    "  .setOutputCol(\"token\")\n",
    " \n",
    "word_embeddings = BertEmbeddings.pretrained('bert_base_cased', 'en')\\\n",
    "  .setInputCols([\"document\", \"token\"])\\\n",
    "  .setOutputCol(\"embeddings\")\n",
    "\n",
    "\n",
    "bert_pipeline = Pipeline().setStages(\n",
    "  [\n",
    "    document_assembler,\n",
    "    tokenizer\n",
    "      #,\n",
    "    #word_embeddings\n",
    "  ]\n",
    ")\n",
    "\n",
    "df_bert = bert_pipeline.fit(df).transform(df)\n",
    "#display(df_bert)\n",
    "df_bert.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: long (nullable = true)\n",
      " |-- document: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      " |-- token: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      " |-- stems_annotations: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      "\n",
      "None\n",
      "+----------------------------------------------------------------------------------+----------------------------------------------------------------------------+\n",
      "|result                                                                            |result                                                                      |\n",
      "+----------------------------------------------------------------------------------+----------------------------------------------------------------------------+\n",
      "|[New, York, is, the, greatest, city, in, the, world, great, Listing, List, Listed]|[new, york, i, the, greatest, citi, in, the, world, great, list, list, list]|\n",
      "|[The, beauty, of, Paris, is, vast]                                                |[the, beauti, of, pari, i, vast]                                            |\n",
      "|[The, Centre, Pompidou, is, in, Paris]                                            |[the, centr, pompid, i, in, pari]                                           |\n",
      "+----------------------------------------------------------------------------------+----------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.annotator import Stemmer, Lemmatizer\n",
    "stemmer = Stemmer().setInputCols(['token']).setOutputCol('stems_annotations')\n",
    "bert_pipeline = Pipeline().setStages(\n",
    "  [\n",
    "    document_assembler,\n",
    "    tokenizer,\n",
    "    stemmer\n",
    "  ]\n",
    ")\n",
    "\n",
    "df_bert = bert_pipeline.fit(df).transform(df)\n",
    "#display(df_bert)\n",
    "#df_bert.show()\n",
    "print(df_bert.printSchema())\n",
    "df_bert.select(\"token.result\",\"stems_annotations.result\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n",
      "+-------------------------------------------------------------------------------+\n",
      "|result                                                                         |\n",
      "+-------------------------------------------------------------------------------+\n",
      "|[New, York, be, the, great, city, in, the, world, great, Listing, List, Listed]|\n",
      "|[The, beauty, of, Paris, be, vast]                                             |\n",
      "|[The, Centre, Pompidou, be, in, Paris]                                         |\n",
      "+-------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = LemmatizerModel.pretrained(name=\"lemma_antbnc\", lang=\"en\").setInputCols(['token']).setOutputCol('lemma_annotations')\n",
    "\n",
    "bert_pipeline = Pipeline().setStages(\n",
    "  [\n",
    "    document_assembler,\n",
    "    tokenizer,\n",
    "    lemmatizer\n",
    "  ]\n",
    ")\n",
    "\n",
    "df_bert = bert_pipeline.fit(df).transform(df)\n",
    "df_bert.select(\"lemma_annotations.result\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---+\n",
      "|        text|  v|\n",
      "+------------+---+\n",
      "|     abc cde|  1|\n",
      "|eefg efa efb|  2|\n",
      "+------------+---+\n",
      "\n",
      "+------------+---+--------------------+--------------------+\n",
      "|        text|  v|            document|               token|\n",
      "+------------+---+--------------------+--------------------+\n",
      "|     abc cde|  1|[[document, 0, 6,...|[[token, 0, 2, ab...|\n",
      "|eefg efa efb|  2|[[document, 0, 11...|[[token, 0, 3, ee...|\n",
      "+------------+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.embeddings import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "import sparknlp\n",
    "spark = sparknlp.start()\n",
    "\n",
    "sentenceDataFrame = spark.createDataFrame([\n",
    "    (\"abc cde\",1),(\"eefg efa efb\",2)\n",
    "], [\"text\",\"v\"])\n",
    "\n",
    "sentenceDataFrame.show()\n",
    "\n",
    "# Spell check (Distance)\n",
    "\n",
    "#df = spark.createDataFrame([(\"abc cde\"),(\"eefg efa efb\")], [\"names\"])\n",
    "\n",
    "nlpPipeline = Pipeline().setStages([\n",
    "  DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\"),\n",
    "  Tokenizer().setInputCols(\"document\").setOutputCol(\"tokens\")])\n",
    "\n",
    "df_bert = bert_pipeline.fit(sentenceDataFrame).transform(sentenceDataFrame)\n",
    "df_bert.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
