{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Pregnancies: integer (nullable = true)\n",
      " |-- Glucose: integer (nullable = true)\n",
      " |-- BloodPressure: integer (nullable = true)\n",
      " |-- SkinThickness: integer (nullable = true)\n",
      " |-- Insulin: integer (nullable = true)\n",
      " |-- BMI: double (nullable = true)\n",
      " |-- DiabetesPedigreeFunction: double (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Outcome: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('ml-bank').getOrCreate()\n",
    "df = spark.read.csv('diabetes.csv', header = True, inferSchema = True)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------------------+\n",
      "|Summary|       Pregnancies|          Glucose|     BloodPressure|\n",
      "+-------+------------------+-----------------+------------------+\n",
      "|  count|               768|              768|               768|\n",
      "|   mean|3.8450520833333335|     120.89453125|       69.10546875|\n",
      "| stddev|  3.36957806269887|31.97261819513622|19.355807170644777|\n",
      "|    min|                 0|                0|                 0|\n",
      "|    max|                17|              199|               122|\n",
      "+-------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.describe().select(\"Summary\",\"Pregnancies\",\"Glucose\",\"BloodPressure\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|Summary|     SkinThickness|           Insulin|\n",
      "+-------+------------------+------------------+\n",
      "|  count|               768|               768|\n",
      "|   mean|20.536458333333332| 79.79947916666667|\n",
      "| stddev|15.952217567727642|115.24400235133803|\n",
      "|    min|                 0|                 0|\n",
      "|    max|                99|               846|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().select(\"Summary\",\"SkinThickness\",\"Insulin\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+\n",
      "|features                                   |\n",
      "+-------------------------------------------+\n",
      "|[6.0,148.0,72.0,35.0,0.0,33.6,0.627,50.0]  |\n",
      "|[1.0,85.0,66.0,29.0,0.0,26.6,0.351,31.0]   |\n",
      "|[8.0,183.0,64.0,0.0,0.0,23.3,0.672,32.0]   |\n",
      "|[1.0,89.0,66.0,23.0,94.0,28.1,0.167,21.0]  |\n",
      "|[0.0,137.0,40.0,35.0,168.0,43.1,2.288,33.0]|\n",
      "|[5.0,116.0,74.0,0.0,0.0,25.6,0.201,30.0]   |\n",
      "|[3.0,78.0,50.0,32.0,88.0,31.0,0.248,26.0]  |\n",
      "|[10.0,115.0,0.0,0.0,0.0,35.3,0.134,29.0]   |\n",
      "|[2.0,197.0,70.0,45.0,543.0,30.5,0.158,53.0]|\n",
      "|[8.0,125.0,96.0,0.0,0.0,0.0,0.232,54.0]    |\n",
      "|[4.0,110.0,92.0,0.0,0.0,37.6,0.191,30.0]   |\n",
      "|[10.0,168.0,74.0,0.0,0.0,38.0,0.537,34.0]  |\n",
      "|[10.0,139.0,80.0,0.0,0.0,27.1,1.441,57.0]  |\n",
      "|[1.0,189.0,60.0,23.0,846.0,30.1,0.398,59.0]|\n",
      "|[5.0,166.0,72.0,19.0,175.0,25.8,0.587,51.0]|\n",
      "|[7.0,100.0,0.0,0.0,0.0,30.0,0.484,32.0]    |\n",
      "|[0.0,118.0,84.0,47.0,230.0,45.8,0.551,31.0]|\n",
      "|[7.0,107.0,74.0,0.0,0.0,29.6,0.254,31.0]   |\n",
      "|[1.0,103.0,30.0,38.0,83.0,43.3,0.183,33.0] |\n",
      "|[1.0,115.0,70.0,30.0,96.0,34.6,0.529,32.0] |\n",
      "+-------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols=df.columns\n",
    "cols.remove(\"Outcome\")\n",
    "# Let us import the vector assembler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=cols,outputCol=\"features\")\n",
    "# Now let us use the transform method to transform our dataset\n",
    "df=assembler.transform(df)\n",
    "df.select(\"features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|            features|Outcome|\n",
      "+--------------------+-------+\n",
      "|[6.0,148.0,72.0,3...|      1|\n",
      "|[1.0,85.0,66.0,29...|      0|\n",
      "|[8.0,183.0,64.0,0...|      1|\n",
      "|[1.0,89.0,66.0,23...|      0|\n",
      "|[0.0,137.0,40.0,3...|      1|\n",
      "|[5.0,116.0,74.0,0...|      0|\n",
      "|[3.0,78.0,50.0,32...|      1|\n",
      "|[10.0,115.0,0.0,0...|      0|\n",
      "|[2.0,197.0,70.0,4...|      1|\n",
      "|[8.0,125.0,96.0,0...|      1|\n",
      "|[4.0,110.0,92.0,0...|      0|\n",
      "|[10.0,168.0,74.0,...|      1|\n",
      "|[10.0,139.0,80.0,...|      0|\n",
      "|[1.0,189.0,60.0,2...|      1|\n",
      "|[5.0,166.0,72.0,1...|      1|\n",
      "|[7.0,100.0,0.0,0....|      1|\n",
      "|[0.0,118.0,84.0,4...|      1|\n",
      "|[7.0,107.0,74.0,0...|      1|\n",
      "|[1.0,103.0,30.0,3...|      0|\n",
      "|[1.0,115.0,70.0,3...|      1|\n",
      "+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"features\",\"Outcome\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------------------+--------------------+----------+\n",
      "|            features|Outcome|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-------+--------------------+--------------------+----------+\n",
      "|[6.0,148.0,72.0,3...|      1|[-0.9530437928800...|[0.27827310281794...|       1.0|\n",
      "|[1.0,85.0,66.0,29...|      0|[2.97341355586980...|[0.95135848467122...|       0.0|\n",
      "|[8.0,183.0,64.0,0...|      1|[-1.3658090656129...|[0.20329779887119...|       1.0|\n",
      "|[1.0,89.0,66.0,23...|      0|[3.13654488056892...|[0.95837526675900...|       0.0|\n",
      "|[0.0,137.0,40.0,3...|      1|[-2.2217341923019...|[0.09781565894362...|       1.0|\n",
      "|[5.0,116.0,74.0,0...|      0|[1.76126995738811...|[0.85336864205596...|       0.0|\n",
      "|[3.0,78.0,50.0,32...|      1|[2.64049246425899...|[0.93342257508603...|       0.0|\n",
      "|[10.0,115.0,0.0,0...|      0|[-0.5952553911576...|[0.35542993414812...|       1.0|\n",
      "|[2.0,197.0,70.0,4...|      1|[-0.8922643314355...|[0.29064277002922...|       1.0|\n",
      "|[8.0,125.0,96.0,0...|      1|[3.27794169861517...|[0.96366428004897...|       0.0|\n",
      "+--------------------+-------+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train=df.select(\"features\",\"Outcome\")\n",
    "test=df.select(\"features\",\"Outcome\")\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(labelCol=\"Outcome\", featuresCol=\"features\")\n",
    "model=lr.fit(train)\n",
    "predict_train=model.transform(train)\n",
    "predict_test=model.transform(test)\n",
    "predict_test.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, spark i j k) --> prob=[0.1596407738787475,0.8403592261212525], prediction=1.000000\n",
      "(2, spark f g h) --> prob=[0.0014541569986709774,0.9985458430013291], prediction=1.000000\n",
      "(5, l m n) --> prob=[0.8378325685476744,0.16216743145232562], prediction=0.000000\n",
      "(6, spark hadoop spark) --> prob=[0.06926633132976037,0.9307336686702395], prediction=1.000000\n",
      "(7, apache hadoop) --> prob=[0.9821575333444218,0.01784246665557808], prediction=0.000000\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "\n",
    "# Prepare training documents from a list of (id, text, label) tuples.\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])\n",
    "\n",
    "# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "model = pipeline.fit(training)\n",
    "\n",
    "# Prepare test documents, which are unlabeled (id, text) tuples.\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (2, \"spark f g h\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"spark hadoop spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "# Make predictions on test documents and print columns of interest.\n",
    "prediction = model.transform(test)\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    rid, text, prob, prediction = row\n",
    "    print(\"(%d, %s) --> prob=%s, prediction=%f\" % (rid, text, str(prob), prediction))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            features|     Scaled_features|\n",
      "+--------------------+--------------------+\n",
      "|[6.0,148.0,72.0,3...|[1.78063837321943...|\n",
      "|[1.0,85.0,66.0,29...|[0.29677306220323...|\n",
      "|[8.0,183.0,64.0,0...|[2.37418449762590...|\n",
      "|[1.0,89.0,66.0,23...|[0.29677306220323...|\n",
      "|[0.0,137.0,40.0,3...|[0.0,4.2849165233...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "standardscaler=StandardScaler().setInputCol(\"features\").setOutputCol(\"Scaled_features\")\n",
    "df=standardscaler.fit(df).transform(df)\n",
    "df.select(\"features\",\"Scaled_features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(labelCol=\"Outcome\", featuresCol=\"Aspect\",weightCol=\"classWeights\",maxIter=10)\n",
    "model=lr.fit(train)\n",
    "predict_train=model.transform(train)\n",
    "predict_test=model.transform(test)\n",
    "predict_test.select(\"Outcome\",\"prediction\").show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
